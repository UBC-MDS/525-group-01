{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099383f9-0260-4fcf-8363-f287e348c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0035b2-48b6-4b9b-aa84-ce2c559d3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshareairline/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836a5e96-d238-48f3-ae6b-593254df8b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 26579150,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'size': 58863,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e'},\n",
       " {'id': 26579171,\n",
       "  'name': 'environment.yml',\n",
       "  'size': 192,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34'},\n",
       " {'id': 26586554,\n",
       "  'name': 'README.md',\n",
       "  'size': 5422,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c'},\n",
       " {'id': 26766812,\n",
       "  'name': 'data.zip',\n",
       "  'size': 814041183,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9'},\n",
       " {'id': 26766815,\n",
       "  'name': 'get_data.py',\n",
       "  'size': 4113,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bfe6787-3f88-4972-b728-77e075d0051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.41 s, sys: 6.87 s, total: 10.3 s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8659db0a-57e7-469c-a247-b61dd7b9c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 1.49 s, total: 11.5 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(os.path.join(output_directory,'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a904cee1-0b03-4381-8c2a-b4a2f98e943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10469384\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   95376895 27 Mar 14:20 MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   94960113 27 Mar 14:20 AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   82474546 27 Mar 14:20 NorESM2-LM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  127613760 27 Mar 14:20 ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  232118894 27 Mar 14:20 FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  330360682 27 Mar 14:20 CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  254009247 27 Mar 14:20 MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  235661418 27 Mar 14:20 GFDL-CM4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  294260911 27 Mar 14:20 BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  295768615 27 Mar 14:20 EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  328852379 27 Mar 14:20 CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   67784105 27 Mar 14:20 NESM3_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   95640682 27 Mar 14:20 MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  114707410 27 Mar 14:20 ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  116179272 27 Mar 14:20 FGOALS-g3_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  102517965 27 Mar 14:20 INM-CM4-8_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  515458033 27 Mar 14:20 MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  332813281 27 Mar 14:20 TaiESM1_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  337555851 27 Mar 14:20 NorESM2-MM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  328787320 27 Mar 14:20 CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff     952202 27 Mar 14:20 observed_daily_rainfall_SYD.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   93829697 27 Mar 14:20 KIOST-ESM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  102692289 27 Mar 14:20 INM-CM5-0_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  206822938 27 Mar 14:20 MIROC6_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   55224437 27 Mar 14:20 BCC-ESM1_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  124586961 27 Mar 14:20 GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   46286371 27 Mar 14:20 CanESM5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  333489879 27 Mar 14:20 SAM0-UNICON_daily_rainfall_NSW.csv\n",
      "drwxr-xr-x@ 30 tanmayagarwal  staff        960 27 Mar 14:20 \u001b[1m\u001b[34m__MACOSX\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr figshareairline/data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "178783d8-4a82-46ba-840d-40de7e0ec1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \"MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09a08e04-c4fe-4a33-8a13-2ec7df8d59cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MPI-ESM-1-2-HAM'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:-23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b28a672-6059-4b60-b7de-e760f11eb21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"_daily_rainfall_NSW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9f05940-4bc5-471c-b5f5-11e86b536c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 807 ms, sys: 72.2 ms, total: 879 ms\n",
      "Wall time: 880 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932835</th>\n",
       "      <td>2014-12-27 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.951144e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932836</th>\n",
       "      <td>2014-12-28 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.257118e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932837</th>\n",
       "      <td>2014-12-29 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>1.204670e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932838</th>\n",
       "      <td>2014-12-30 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.632404e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932839</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>3.431610e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932840 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0        1889-01-01 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "1        1889-01-02 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "2        1889-01-03 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "3        1889-01-04 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "4        1889-01-05 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "...                      ...      ...      ...      ...      ...   \n",
       "1932835  2014-12-27 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932836  2014-12-28 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932837  2014-12-29 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932838  2014-12-30 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932839  2014-12-31 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "\n",
       "         rain (mm/day)  \n",
       "0         3.293256e-13  \n",
       "1         0.000000e+00  \n",
       "2         0.000000e+00  \n",
       "3         0.000000e+00  \n",
       "4         1.047658e-02  \n",
       "...                ...  \n",
       "1932835   2.951144e-02  \n",
       "1932836   2.257118e-01  \n",
       "1932837   1.204670e-01  \n",
       "1932838   2.632404e-02  \n",
       "1932839   3.431610e-02  \n",
       "\n",
       "[1932840 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### just listing to get an idea how individual file looks like \n",
    "use_cols = ['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "df = pd.read_csv(\"figshareairline/data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=use_cols,dtype={'TailNum': 'str'})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cb23fb8-14f5-45c9-adb8-0fb2224207f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10469384\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   95376895 27 Mar 14:20 MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   94960113 27 Mar 14:20 AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   82474546 27 Mar 14:20 NorESM2-LM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  127613760 27 Mar 14:20 ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  232118894 27 Mar 14:20 FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  330360682 27 Mar 14:20 CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  254009247 27 Mar 14:20 MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  235661418 27 Mar 14:20 GFDL-CM4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  294260911 27 Mar 14:20 BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  295768615 27 Mar 14:20 EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  328852379 27 Mar 14:20 CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   67784105 27 Mar 14:20 NESM3_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   95640682 27 Mar 14:20 MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  114707410 27 Mar 14:20 ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  116179272 27 Mar 14:20 FGOALS-g3_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  102517965 27 Mar 14:20 INM-CM4-8_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  515458033 27 Mar 14:20 MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  332813281 27 Mar 14:20 TaiESM1_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  337555851 27 Mar 14:20 NorESM2-MM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  328787320 27 Mar 14:20 CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff     952202 27 Mar 14:20 observed_daily_rainfall_SYD.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   93829697 27 Mar 14:20 KIOST-ESM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  102692289 27 Mar 14:20 INM-CM5-0_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  206822938 27 Mar 14:20 MIROC6_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   55224437 27 Mar 14:20 BCC-ESM1_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  124586961 27 Mar 14:20 GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff   46286371 27 Mar 14:20 CanESM5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--@  1 tanmayagarwal  staff  333489879 27 Mar 14:20 SAM0-UNICON_daily_rainfall_NSW.csv\n",
      "drwxr-xr-x@ 30 tanmayagarwal  staff        960 27 Mar 14:20 \u001b[1m\u001b[34m__MACOSX\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr figshareairline/data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84aecbfd-4b90-420b-9dd6-4987e803c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 1s, sys: 10.7 s, total: 5min 11s\n",
      "Wall time: 5min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## here we are using a normal python way for merging the data \n",
    "import pandas as pd\n",
    "files = glob.glob('figshareairline/data/*.csv')\n",
    "df = pd.concat(pd.read_csv(file, index_col=0, parse_dates=True, \n",
    "            usecols=use_cols).assign(model = file[21:-23])\n",
    "          for file in files)\n",
    "              \n",
    "df.to_csv(\"figshareairline/combined_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6638a87-4d0d-4d25-9e48-2ce4e79fad2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1889-01-01 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-02 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-03 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-04 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-05 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-27 12:00:00</th>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>6.689683e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-28 12:00:00</th>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>7.862555e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-29 12:00:00</th>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>1.000503e+01</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-30 12:00:00</th>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>8.541592e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 12:00:00</th>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>6.811749e+01</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62467843 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       lat_min    lat_max   lon_min   lon_max  rain (mm/day)  \\\n",
       "time                                                                           \n",
       "1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.244226e-13   \n",
       "1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.217326e-13   \n",
       "1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.498125e-13   \n",
       "1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.251282e-13   \n",
       "1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.270161e-13   \n",
       "...                        ...        ...       ...       ...            ...   \n",
       "2014-12-27 12:00:00 -30.157068 -29.214660  153.1250  154.3750   6.689683e+00   \n",
       "2014-12-28 12:00:00 -30.157068 -29.214660  153.1250  154.3750   7.862555e+00   \n",
       "2014-12-29 12:00:00 -30.157068 -29.214660  153.1250  154.3750   1.000503e+01   \n",
       "2014-12-30 12:00:00 -30.157068 -29.214660  153.1250  154.3750   8.541592e+00   \n",
       "2014-12-31 12:00:00 -30.157068 -29.214660  153.1250  154.3750   6.811749e+01   \n",
       "\n",
       "                               model  \n",
       "time                                  \n",
       "1889-01-01 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-02 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-03 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-04 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-05 12:00:00  MPI-ESM-1-2-HAM  \n",
       "...                              ...  \n",
       "2014-12-27 12:00:00      SAM0-UNICON  \n",
       "2014-12-28 12:00:00      SAM0-UNICON  \n",
       "2014-12-29 12:00:00      SAM0-UNICON  \n",
       "2014-12-30 12:00:00      SAM0-UNICON  \n",
       "2014-12-31 12:00:00      SAM0-UNICON  \n",
       "\n",
       "[62467843 rows x 6 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27366376-859b-4399-ac82-154b081a11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigshareairline/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figshareairline/combined_data.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f40627a5-b6bb-408e-b50f-89b19a3f0ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62467843, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15cee43d-1331-4d08-8d06-8f0fb2e2835a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)', 'model'], dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc7eb3-a493-442d-96a8-7de6281d3766",
   "metadata": {},
   "source": [
    "### Trying to load data to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd88e6d3-35d3-4623-a0d9-67f6ce3026c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"figshareairline/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f495ce96-ef74-4392-b774-270c2973a37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3498.199336"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage().sum()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e707299f-0a91-4884-875a-aafd0998193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using less columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ec33444-cea3-49bb-9d2f-7db75d5ce7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"figshareairline/combined_data.csv\", usecols=['time', 'rain (mm/day)','model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50f64a77-e611-404d-a8e9-efed821ac306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499.22836"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage().sum()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75ee3bbb-5c8d-4c89-bdfe-f141adaed337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dtype to reduce size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96af4b20-0026-4a03-a7ee-8375b1a2a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d74728e2-a4c2-47fb-9a3d-bf9bcd410161",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'lat_min': np.float16,\n",
    "    'lat_max': np.float16,\n",
    "    'lon_min': np.float16,\n",
    "    'lon_max': np.float16,\n",
    "    'rain (mm/day)': np.float32,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "427abc31-1361-41e0-8991-bca47492de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"figshareairline/combined_data.csv\", dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1dc062b3-d325-4ce6-aa77-969391a7e815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749.099732"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage().sum()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d93e0727-9cd1-4d02-b689-1eaa8c5acc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.4375</td>\n",
       "      <td>-33.5625</td>\n",
       "      <td>141.5</td>\n",
       "      <td>143.5</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.4375</td>\n",
       "      <td>-33.5625</td>\n",
       "      <td>141.5</td>\n",
       "      <td>143.5</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.4375</td>\n",
       "      <td>-33.5625</td>\n",
       "      <td>141.5</td>\n",
       "      <td>143.5</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.4375</td>\n",
       "      <td>-33.5625</td>\n",
       "      <td>141.5</td>\n",
       "      <td>143.5</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.4375</td>\n",
       "      <td>-33.5625</td>\n",
       "      <td>141.5</td>\n",
       "      <td>143.5</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  lat_min  lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "0  1889-01-01 12:00:00 -35.4375 -33.5625    141.5    143.5   4.244226e-13   \n",
       "1  1889-01-02 12:00:00 -35.4375 -33.5625    141.5    143.5   4.217326e-13   \n",
       "2  1889-01-03 12:00:00 -35.4375 -33.5625    141.5    143.5   4.498125e-13   \n",
       "3  1889-01-04 12:00:00 -35.4375 -33.5625    141.5    143.5   4.251282e-13   \n",
       "4  1889-01-05 12:00:00 -35.4375 -33.5625    141.5    143.5   4.270161e-13   \n",
       "\n",
       "             model  \n",
       "0  MPI-ESM-1-2-HAM  \n",
       "1  MPI-ESM-1-2-HAM  \n",
       "2  MPI-ESM-1-2-HAM  \n",
       "3  MPI-ESM-1-2-HAM  \n",
       "4  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c037c13a-98da-4d5d-ac26-8baaf6c263ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   time           object \n",
      " 1   lat_min        float16\n",
      " 2   lat_max        float16\n",
      " 3   lon_min        float16\n",
      " 4   lon_max        float16\n",
      " 5   rain (mm/day)  float32\n",
      " 6   model          object \n",
      "dtypes: float16(4), float32(1), object(2)\n",
      "memory usage: 1.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "870d5628-2f0d-46b9-a0fb-063419dbcc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdelimiter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int | Sequence[int] | None | Literal['infer']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[Hashable] | None | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindex_col\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'IndexLabel | Literal[False] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msqueeze\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmangle_dupe_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DtypeArg | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'CSVEngine | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrue_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfalse_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskipfooter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mna_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_blank_lines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minfer_datetime_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_date_col\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdate_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdayfirst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'CompressionOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthousands\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding_errors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdialect\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | csv.Dialect | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarn_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfloat_precision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['high', 'legacy'] | None\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DataFrame | TextFileReader'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read a comma-separated values (csv) file into DataFrame.\n",
       "\n",
       "Also supports optionally iterating or breaking of the file\n",
       "into chunks.\n",
       "\n",
       "Additional help can be found in the online docs for\n",
       "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "filepath_or_buffer : str, path object or file-like object\n",
       "    Any valid string path is acceptable. The string could be a URL. Valid\n",
       "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
       "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
       "\n",
       "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
       "\n",
       "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
       "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
       "sep : str, default ','\n",
       "    Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
       "    the separator, but the Python parsing engine can, meaning the latter will\n",
       "    be used and automatically detect the separator by Python's builtin sniffer\n",
       "    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
       "    different from ``'\\s+'`` will be interpreted as regular expressions and\n",
       "    will also force the use of the Python parsing engine. Note that regex\n",
       "    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
       "delimiter : str, default ``None``\n",
       "    Alias for sep.\n",
       "header : int, list of int, None, default 'infer'\n",
       "    Row number(s) to use as the column names, and the start of the\n",
       "    data.  Default behavior is to infer the column names: if no names\n",
       "    are passed the behavior is identical to ``header=0`` and column\n",
       "    names are inferred from the first line of the file, if column\n",
       "    names are passed explicitly then the behavior is identical to\n",
       "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
       "    replace existing names. The header can be a list of integers that\n",
       "    specify row locations for a multi-index on the columns\n",
       "    e.g. [0,1,3]. Intervening rows that are not specified will be\n",
       "    skipped (e.g. 2 in this example is skipped). Note that this\n",
       "    parameter ignores commented lines and empty lines if\n",
       "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
       "    data rather than the first line of the file.\n",
       "names : array-like, optional\n",
       "    List of column names to use. If the file contains a header row,\n",
       "    then you should explicitly pass ``header=0`` to override the column names.\n",
       "    Duplicates in this list are not allowed.\n",
       "index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
       "  Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
       "  string name or column index. If a sequence of int / str is given, a\n",
       "  MultiIndex is used.\n",
       "\n",
       "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
       "  column as the index, e.g. when you have a malformed file with delimiters at\n",
       "  the end of each line.\n",
       "usecols : list-like or callable, optional\n",
       "    Return a subset of the columns. If list-like, all elements must either\n",
       "    be positional (i.e. integer indices into the document columns) or strings\n",
       "    that correspond to column names provided either by the user in `names` or\n",
       "    inferred from the document header row(s). If ``names`` are given, the document\n",
       "    header row(s) are not taken into account. For example, a valid list-like\n",
       "    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
       "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
       "    To instantiate a DataFrame from ``data`` with element order preserved use\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
       "    in ``['foo', 'bar']`` order or\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
       "    for ``['bar', 'foo']`` order.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the column\n",
       "    names, returning names where the callable function evaluates to True. An\n",
       "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
       "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
       "    parsing time and lower memory usage.\n",
       "squeeze : bool, default False\n",
       "    If the parsed data only contains one column then return a Series.\n",
       "\n",
       "    .. deprecated:: 1.4.0\n",
       "        Append ``.squeeze(\"columns\")`` to the call to ``read_csv`` to squeeze\n",
       "        the data.\n",
       "prefix : str, optional\n",
       "    Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
       "\n",
       "    .. deprecated:: 1.4.0\n",
       "       Use a list comprehension on the DataFrame's columns after calling ``read_csv``.\n",
       "mangle_dupe_cols : bool, default True\n",
       "    Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
       "    'X'...'X'. Passing in False will cause data to be overwritten if there\n",
       "    are duplicate names in the columns.\n",
       "\n",
       "    .. deprecated:: 1.5.0\n",
       "        Not implemented, and a new argument to specify the pattern for the\n",
       "        names of duplicated columns will be added instead\n",
       "dtype : Type name or dict of column -> type, optional\n",
       "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
       "    'c': 'Int64'}\n",
       "    Use `str` or `object` together with suitable `na_values` settings\n",
       "    to preserve and not interpret dtype.\n",
       "    If converters are specified, they will be applied INSTEAD\n",
       "    of dtype conversion.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "\n",
       "        Support for defaultdict was added. Specify a defaultdict as input where\n",
       "        the default determines the dtype of the columns which are not explicitly\n",
       "        listed.\n",
       "engine : {'c', 'python', 'pyarrow'}, optional\n",
       "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
       "    is currently more feature-complete. Multithreading is currently only supported by\n",
       "    the pyarrow engine.\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
       "        are unsupported, or may not work correctly, with this engine.\n",
       "converters : dict, optional\n",
       "    Dict of functions for converting values in certain columns. Keys can either\n",
       "    be integers or column labels.\n",
       "true_values : list, optional\n",
       "    Values to consider as True.\n",
       "false_values : list, optional\n",
       "    Values to consider as False.\n",
       "skipinitialspace : bool, default False\n",
       "    Skip spaces after delimiter.\n",
       "skiprows : list-like, int or callable, optional\n",
       "    Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
       "    at the start of the file.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the row\n",
       "    indices, returning True if the row should be skipped and False otherwise.\n",
       "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
       "skipfooter : int, default 0\n",
       "    Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
       "nrows : int, optional\n",
       "    Number of rows of file to read. Useful for reading pieces of large files.\n",
       "na_values : scalar, str, list-like, or dict, optional\n",
       "    Additional strings to recognize as NA/NaN. If dict passed, specific\n",
       "    per-column NA values.  By default the following values are interpreted as\n",
       "    NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
       "    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
       "    'nan', 'null'.\n",
       "keep_default_na : bool, default True\n",
       "    Whether or not to include the default NaN values when parsing the data.\n",
       "    Depending on whether `na_values` is passed in, the behavior is as follows:\n",
       "\n",
       "    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
       "      is appended to the default NaN values used for parsing.\n",
       "    * If `keep_default_na` is True, and `na_values` are not specified, only\n",
       "      the default NaN values are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are specified, only\n",
       "      the NaN values specified `na_values` are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are not specified, no\n",
       "      strings will be parsed as NaN.\n",
       "\n",
       "    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
       "    `na_values` parameters will be ignored.\n",
       "na_filter : bool, default True\n",
       "    Detect missing value markers (empty strings and the value of na_values). In\n",
       "    data without any NAs, passing na_filter=False can improve the performance\n",
       "    of reading a large file.\n",
       "verbose : bool, default False\n",
       "    Indicate number of NA values placed in non-numeric columns.\n",
       "skip_blank_lines : bool, default True\n",
       "    If True, skip over blank lines rather than interpreting as NaN values.\n",
       "parse_dates : bool or list of int or names or list of lists or dict, default False\n",
       "    The behavior is as follows:\n",
       "\n",
       "    * boolean. If True -> try parsing the index.\n",
       "    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
       "      each as a separate date column.\n",
       "    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
       "      a single date column.\n",
       "    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
       "      result 'foo'\n",
       "\n",
       "    If a column or index cannot be represented as an array of datetimes,\n",
       "    say because of an unparsable value or a mixture of timezones, the column\n",
       "    or index will be returned unaltered as an object data type. For\n",
       "    non-standard datetime parsing, use ``pd.to_datetime`` after\n",
       "    ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
       "    specify ``date_parser`` to be a partially-applied\n",
       "    :func:`pandas.to_datetime` with ``utc=True``. See\n",
       "    :ref:`io.csv.mixed_timezones` for more.\n",
       "\n",
       "    Note: A fast-path exists for iso8601-formatted dates.\n",
       "infer_datetime_format : bool, default False\n",
       "    If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
       "    format of the datetime strings in the columns, and if it can be inferred,\n",
       "    switch to a faster method of parsing them. In some cases this can increase\n",
       "    the parsing speed by 5-10x.\n",
       "keep_date_col : bool, default False\n",
       "    If True and `parse_dates` specifies combining multiple columns then\n",
       "    keep the original columns.\n",
       "date_parser : function, optional\n",
       "    Function to use for converting a sequence of string columns to an array of\n",
       "    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
       "    conversion. Pandas will try to call `date_parser` in three different ways,\n",
       "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
       "    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
       "    string values from the columns defined by `parse_dates` into a single array\n",
       "    and pass that; and 3) call `date_parser` once for each row using one or\n",
       "    more strings (corresponding to the columns defined by `parse_dates`) as\n",
       "    arguments.\n",
       "dayfirst : bool, default False\n",
       "    DD/MM format dates, international and European format.\n",
       "cache_dates : bool, default True\n",
       "    If True, use a cache of unique, converted dates to apply the datetime\n",
       "    conversion. May produce significant speed-up when parsing duplicate\n",
       "    date strings, especially ones with timezone offsets.\n",
       "\n",
       "    .. versionadded:: 0.25.0\n",
       "iterator : bool, default False\n",
       "    Return TextFileReader object for iteration or getting chunks with\n",
       "    ``get_chunk()``.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       ``TextFileReader`` is a context manager.\n",
       "chunksize : int, optional\n",
       "    Return TextFileReader object for iteration.\n",
       "    See the `IO Tools docs\n",
       "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
       "    for more information on ``iterator`` and ``chunksize``.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       ``TextFileReader`` is a context manager.\n",
       "compression : str or dict, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
       "    path-like, then detect compression from the following extensions: '.gz',\n",
       "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
       "    (otherwise no compression).\n",
       "    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
       "    Set to ``None`` for no decompression.\n",
       "    Can also be a dict with key ``'method'`` set\n",
       "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
       "    key-value pairs are forwarded to\n",
       "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
       "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor`` or\n",
       "    ``tarfile.TarFile``, respectively.\n",
       "    As an example, the following could be passed for Zstandard decompression using a\n",
       "    custom compression dictionary:\n",
       "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
       "\n",
       "        .. versionadded:: 1.5.0\n",
       "            Added support for `.tar` files.\n",
       "\n",
       "    .. versionchanged:: 1.4.0 Zstandard support.\n",
       "\n",
       "thousands : str, optional\n",
       "    Thousands separator.\n",
       "decimal : str, default '.'\n",
       "    Character to recognize as decimal point (e.g. use ',' for European data).\n",
       "lineterminator : str (length 1), optional\n",
       "    Character to break file into lines. Only valid with C parser.\n",
       "quotechar : str (length 1), optional\n",
       "    The character used to denote the start and end of a quoted item. Quoted\n",
       "    items can include the delimiter and it will be ignored.\n",
       "quoting : int or csv.QUOTE_* instance, default 0\n",
       "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
       "    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
       "doublequote : bool, default ``True``\n",
       "   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
       "   whether or not to interpret two consecutive quotechar elements INSIDE a\n",
       "   field as a single ``quotechar`` element.\n",
       "escapechar : str (length 1), optional\n",
       "    One-character string used to escape other characters.\n",
       "comment : str, optional\n",
       "    Indicates remainder of line should not be parsed. If found at the beginning\n",
       "    of a line, the line will be ignored altogether. This parameter must be a\n",
       "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
       "    fully commented lines are ignored by the parameter `header` but not by\n",
       "    `skiprows`. For example, if ``comment='#'``, parsing\n",
       "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
       "    treated as the header.\n",
       "encoding : str, optional\n",
       "    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
       "    standard encodings\n",
       "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
       "       ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
       "       This behavior was previously only the case for ``engine=\"python\"``.\n",
       "\n",
       "    .. versionchanged:: 1.3.0\n",
       "\n",
       "       ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
       "       influence on how encoding errors are handled.\n",
       "\n",
       "encoding_errors : str, optional, default \"strict\"\n",
       "    How encoding errors are treated. `List of possible values\n",
       "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "dialect : str or csv.Dialect, optional\n",
       "    If provided, this parameter will override values (default or not) for the\n",
       "    following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
       "    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
       "    override values, a ParserWarning will be issued. See csv.Dialect\n",
       "    documentation for more details.\n",
       "error_bad_lines : bool, optional, default ``None``\n",
       "    Lines with too many fields (e.g. a csv line with too many commas) will by\n",
       "    default cause an exception to be raised, and no DataFrame will be returned.\n",
       "    If False, then these \"bad lines\" will be dropped from the DataFrame that is\n",
       "    returned.\n",
       "\n",
       "    .. deprecated:: 1.3.0\n",
       "       The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
       "       encountering a bad line instead.\n",
       "warn_bad_lines : bool, optional, default ``None``\n",
       "    If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
       "    \"bad line\" will be output.\n",
       "\n",
       "    .. deprecated:: 1.3.0\n",
       "       The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
       "       encountering a bad line instead.\n",
       "on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
       "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
       "    Allowed values are :\n",
       "\n",
       "        - 'error', raise an Exception when a bad line is encountered.\n",
       "        - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
       "        - 'skip', skip bad lines without raising or warning when they are encountered.\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        - callable, function with signature\n",
       "          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
       "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
       "          If the function returns ``None``, the bad line will be ignored.\n",
       "          If the function returns a new list of strings with more elements than\n",
       "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
       "          Only supported when ``engine=\"python\"``\n",
       "\n",
       "delim_whitespace : bool, default False\n",
       "    Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
       "    used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
       "    is set to True, nothing should be passed in for the ``delimiter``\n",
       "    parameter.\n",
       "low_memory : bool, default True\n",
       "    Internally process the file in chunks, resulting in lower memory use\n",
       "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
       "    types either set False, or specify the type with the `dtype` parameter.\n",
       "    Note that the entire file is read into a single DataFrame regardless,\n",
       "    use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
       "    (Only valid with C parser).\n",
       "memory_map : bool, default False\n",
       "    If a filepath is provided for `filepath_or_buffer`, map the file object\n",
       "    directly onto memory and access the data directly from there. Using this\n",
       "    option can improve performance because there is no longer any I/O overhead.\n",
       "float_precision : str, optional\n",
       "    Specifies which converter the C engine should use for floating-point\n",
       "    values. The options are ``None`` or 'high' for the ordinary converter,\n",
       "    'legacy' for the original lower precision pandas converter, and\n",
       "    'round_trip' for the round-trip converter.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "    .. versionadded:: 1.2\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or TextParser\n",
       "    A comma-separated values (csv) file is returned as two-dimensional\n",
       "    data structure with labeled axes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
       "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
       "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/miniconda3/envs/525_2023/lib/python3.10/site-packages/pandas/io/parsers/readers.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.read_csv(\"figshareairline/combined_data.csv\", usecols=['timestamp', 'rain (mm/day)','model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006deb0-0366-4d0c-b76d-bb01c279a137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525_2023]",
   "language": "python",
   "name": "conda-env-525_2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
